{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/autofs/cluster/octdata2/users/epc28/veritas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/autofs/cluster/octdata2/users/epc28/miniconda/envs/vesselsynth/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/autofs/cluster/octdata2/users/epc28/miniconda/envs/vesselsynth/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c106detail23torchInternalAssertFailEPKcS2_jS2_RKSs'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchmetrics.functional import dice, jaccard_index\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"cornucopia\")\n",
    "import cornucopia as cc\n",
    "\n",
    "sys.path.append(\"veritas\")\n",
    "from veritas import models\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/autofs/cluster/octdata2/users/epc28/veritas/output/models/version_2\n"
     ]
    }
   ],
   "source": [
    "paths = {\n",
    "    \"checkpoint\": \"/autofs/cluster/octdata2/users/epc28/veritas/output/models/version_2/checkpoints/epoch=117-val_loss=0.00095.ckpt\",           # string (path)\n",
    "    #\"checkpoint\": \"/autofs/cluster/octdata2/users/epc28/veritas/output/models/version_3/checkpoints/epoch=168-val_loss=0.00328.ckpt\",\n",
    "    \"image_volume\": \"output/models/version_2/predictions/dylan_data/I_mosaic_1_1_0.nii\",                                                        # string (path)\n",
    "    \"ground_truth\": \"/autofs/cluster/octdata2/users/epc28/veritas/output/models/version_2/predictions/dylan_data/ground_truth.nii\",             # string (path) or None\n",
    "    \"save_dir\": \"/autofs/cluster/octdata2/users/epc28/veritas/output/models/version_2/predictions/dylan_data/\",                                 # string (path)\n",
    "}\n",
    "\n",
    "# Don't change this model path\n",
    "paths[\"model_path\"] = \"/\".join(paths[\"checkpoint\"].split('/')[:-2])\n",
    "print(paths['model_path'])\n",
    "\n",
    "\n",
    "prediction_settings = {\n",
    "    \"step_size\": 256,\n",
    "    \"padding_method\": \"reflect\",\n",
    "    \"device\": \"cuda\"\n",
    "}\n",
    "\n",
    "options = {\n",
    "    \"threshold\": False,\n",
    "    \"fixed_threshold\": False,\n",
    "    \"compute_metric\": False,\n",
    "}\n",
    "\n",
    "if options['threshold'] == True:\n",
    "\n",
    "    if options[\"fixed_threshold\"] == True:\n",
    "        threshold_settings = {\n",
    "            \"threshold\": 0.5\n",
    "            }\n",
    "        \n",
    "    elif options[\"fixed_threshold\"] == False:\n",
    "        threshold_settings = {\n",
    "            \"start\": 0.05,\n",
    "            \"stop\": 0.95,\n",
    "            \"step\": 0.05,\n",
    "            }\n",
    "    else:\n",
    "        print(\"I don't know how to threshold!\")\n",
    "\n",
    "if options[\"compute_metric\"] == True:\n",
    "    metric_type = \"dice\"                 # string (\"dice\" or \"iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OctVolume(Dataset):\n",
    "\n",
    "    def __init__(self, volume_path, trainee, tile_size, step_size, device=\"cpu\", subset=-1, transform=None, target_transform=None):\n",
    "        self.volume_path = volume_path\n",
    "        self.device = device\n",
    "        self.tile_size = tile_size\n",
    "        self.step_size = step_size\n",
    "        self.volume_dtype = torch.float16\n",
    "        self.imprint_dtype = torch.float16\n",
    "        self.trainee = trainee\n",
    "        \n",
    "        # Get all volume specific things\n",
    "        with torch.no_grad():\n",
    "            self.volume_nifti = nib.load(self.volume_path)\n",
    "            self.volume_affine = self.volume_nifti.affine\n",
    "            self.volume_header = self.volume_nifti.header\n",
    "            self.volume_tensor = torch.tensor(self.volume_nifti.get_fdata(), dtype=self.volume_dtype, device=self.device)\n",
    "            self.volume_tensor = cc.QuantileTransform(pmin=0, pmax=1, vmin=0.05, vmax=0.95, clamp=False)(self.volume_tensor.to(torch.float) + 0.000001).to(self.volume_dtype)\n",
    "            self.raw_volume_shape = self.volume_tensor.shape    \n",
    "        # Pad each dimension individually\n",
    "        self.pad_dimension()\n",
    "        self.imprint_tensor = torch.zeros(self.volume_tensor.shape, dtype=self.imprint_dtype, device=self.device)\n",
    "        # Partition volume into overlapping 3d patches\n",
    "        self.get_frame_coords(step_size=self.step_size)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coordlist)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        working_coords = self.coordlist[idx]\n",
    "        x_slice = slice(*working_coords[0])\n",
    "        y_slice = slice(*working_coords[1])\n",
    "        z_slice = slice(*working_coords[2])\n",
    "        tile = self.volume_tensor[x_slice, y_slice, z_slice].to(self.volume_dtype).detach().to(\"cuda\").to(torch.float)#.to('cpu')\n",
    "        prediction = self.trainee(tile.unsqueeze(0).unsqueeze(0))#.to('cpu')\n",
    "        prediction = torch.sigmoid(prediction).squeeze().squeeze().detach()\n",
    "        self.imprint_tensor[x_slice, y_slice, z_slice] += prediction\n",
    "        return tile, prediction\n",
    "\n",
    "\n",
    "    def predict(self):\n",
    "        '''Predict on all patches within 3d volume via getitem function. Normalize resultant imprint and strip padding.'''\n",
    "        # Normalizing\n",
    "        length = self.__len__()\n",
    "        print(\"Predicting on\", length, 'patches')\n",
    "        for i in range(length):\n",
    "            self.__getitem__(i)\n",
    "            sys.stdout.write(f\"\\rPrediction {i + 1}/{length}\")\n",
    "            sys.stdout.flush()\n",
    "        s = slice(self.tile_size, -self.tile_size)\n",
    "        self.volume_tensor = self.volume_tensor[s, s, s]\n",
    "        self.imprint_tensor = self.imprint_tensor[s, s, s]\n",
    "\n",
    "\n",
    "    def pad_dimension(self):\n",
    "        with torch.no_grad():\n",
    "            self.volume_tensor = self.volume_tensor.clone().detach().unsqueeze(0)\n",
    "            if len(self.volume_tensor.shape) == 4:\n",
    "                padding = torch.ones(1, 6, dtype=torch.int) * self.tile_size\n",
    "                padding = tuple(*padding)\n",
    "                self.volume_tensor = torch.nn.functional.pad(self.volume_tensor, padding, 'replicate').squeeze()\n",
    "            else:\n",
    "                print('Input tensor has shape', self.volume_tensor.shape)\n",
    "\n",
    "\n",
    "    def get_frame_coords(self, step_size):\n",
    "        coords = []\n",
    "        for dim in range(3):\n",
    "            dim_start_frame = list(np.arange(0, self.volume_tensor.shape[dim] - self.tile_size, step_size))\n",
    "            # Remove all elements from starting frame list if all they're going to get is padding\n",
    "            dim_start_frame.remove(0)\n",
    "            # Remove all elements from starting frame list if all they're going to get is padding\n",
    "            dim_end_frame = [d + self.tile_size for d in dim_start_frame]\n",
    "            coords.append(zip(dim_start_frame, dim_end_frame))\n",
    "        for dim in range(len(coords)):\n",
    "            if dim == 0:\n",
    "                self.x_coords = [i for i in coords[dim]]\n",
    "            if dim == 1:\n",
    "                self.y_coords = [i for i in coords[dim]]\n",
    "            if dim == 2:\n",
    "                self.z_coords = [i for i in coords[dim]]\n",
    "        self.coordlist = []\n",
    "        try:\n",
    "            for x in self.x_coords:\n",
    "                for y in self.y_coords:\n",
    "                    for z in self.z_coords:\n",
    "                        self.coordlist.append([x, y, z])\n",
    "        except:\n",
    "            for x in self.x_coords:\n",
    "                for y in self.y_coords:\n",
    "                    self.coordlist.append([x, y])\n",
    "\n",
    "                    \n",
    "def findthresh(prediction, ground_truth, thresh_start, thresh_stop, thresh_step):\n",
    "    #prediction = prediction / torch.max(prediction)\n",
    "    threshold_lst = np.arange(thresh_start, thresh_stop, thresh_step)\n",
    "    lst = []\n",
    "    for thresh in threshold_lst:\n",
    "        prediction_temp = prediction.clone()\n",
    "        prediction_temp[prediction_temp >= thresh] = 1\n",
    "        prediction_temp[prediction_temp < thresh] = 0\n",
    "        if metric_type == 'dice':\n",
    "            metric = dice(prediction_temp, ground_truth, multiclass=False)\n",
    "        elif metric_type == \"iou\":\n",
    "            metric = jaccard_index(preds=prediction_temp, target=ground_truth, task=\"binary\")\n",
    "        else:\n",
    "            print(\"I don't know that metric!\")\n",
    "        lst.append(metric.tolist())\n",
    "        \n",
    "    mx = max(lst)\n",
    "    mx_idx = lst.index(mx)\n",
    "    return threshold_lst[mx_idx], lst[mx_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaging by: 1.0\n",
      "Predicting on 100 patches\n",
      "Prediction 100/100\n",
      "Saving to: /autofs/cluster/octdata2/users/epc28/veritas/output/models/version_2/predictions/dylan_data//prediction_stepsz-256_prob-map.nii\n",
      "\n",
      "Process took 104.35 [sec]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    factors = {256 : 0,\n",
    "               128: 1,\n",
    "               64: 2}\n",
    "    \n",
    "    averaging_factor = 1 / (8 ** factors[prediction_settings[\"step_size\"]])\n",
    "    print(\"Averaging by:\", averaging_factor)\n",
    "\n",
    "    t1 = time.time()\n",
    "    \n",
    "    volume_path = paths[\"image_volume\"]\n",
    "    model_path = paths[\"model_path\"]\n",
    "\n",
    "    unet = models.UNet(model_path, paths[\"checkpoint\"])\n",
    "    oct = OctVolume(volume_path, unet.trainee, tile_size=unet.model_params['data']['shape'], step_size=prediction_settings[\"step_size\"], device=prediction_settings[\"device\"])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        oct.predict()\n",
    "        #x = oct.volume_tensor\n",
    "        y = oct.imprint_tensor \n",
    "        y = y * averaging_factor\n",
    "        y = y / torch.max(y)\n",
    "\n",
    "        savedir = paths[\"save_dir\"]\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "        if isinstance(paths[\"ground_truth\"], str):\n",
    "            ground_truth_nifti = nib.load(paths[\"ground_truth\"])\n",
    "            ground_truth_tensor = torch.tensor(ground_truth_nifti.get_fdata()).to(\"cuda\")\n",
    "            ground_truth_tensor[ground_truth_tensor >= 0.5] = 1\n",
    "            ground_truth_tensor[ground_truth_tensor <= 0.5] = 0\n",
    "            ground_truth_tensor = ground_truth_tensor.to(torch.bool)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "##########################################################\n",
    "        if options[\"threshold\"] == True:\n",
    "            if options[\"fixed_threshold\"] == True:\n",
    "                threshold = threshold_settings[\"threshold\"]\n",
    "            elif options[\"fixed_threshold\"] == False:\n",
    "                if isinstance(paths[\"ground_truth\"], str):\n",
    "                    threshold, accuracy = findthresh(y, ground_truth_tensor, threshold_settings[\"start\"], threshold_settings[\"stop\"], threshold_settings[\"step\"])\n",
    "                else:\n",
    "                    print(\"No ground truth!! Can't compute best threshold :(\")\n",
    "            y[y >= threshold] = 1\n",
    "            y[y < threshold] = 0\n",
    "            y = y.to(torch.bool)\n",
    "            human_threshold = round(threshold, 3)\n",
    "            out_file = f\"prediction_stepsz-{prediction_settings['step_size']}_thresh-{human_threshold}.nii\"\n",
    "            if options[\"compute_metric\"] == True:\n",
    "                accuracy = dice(preds=y, target=ground_truth_tensor, multiclass=False)\n",
    "                human_accuracy = round(accuracy.item(), 3)\n",
    "                out_file = f\"prediction_stepsz-{prediction_settings['step_size']}_thresh-{human_threshold}_dice-{human_accuracy}.nii\"\n",
    "        elif options[\"threshold\"] == False:\n",
    "            out_file = f\"prediction_stepsz-{prediction_settings['step_size']}_prob-map.nii\"\n",
    "        else:\n",
    "            print(\"IDK what that threshold is!!!\")\n",
    "        out_file_abspath = f\"{paths['save_dir']}/{out_file}\"\n",
    "        print(f\"\\nSaving to: {out_file_abspath}\")\n",
    "        nifti = nib.nifti1.Nifti1Image(y.cpu().numpy(), affine=oct.volume_affine, header=oct.volume_header, dtype=np.uint8)\n",
    "        nib.save(nifti , out_file_abspath)\n",
    "        \n",
    "##########################################################\n",
    "        \n",
    "    t2 = time.time()\n",
    "    print(f\"\\nProcess took {round(t2 - t1, 2)} [sec]\")\n",
    "\n",
    "    #plt.figure()\n",
    "    ##subplot(r,c) provide the no. of rows and columns\n",
    "    #f, axarr = plt.subplots(1, 3, figsize=(20, 20), constrained_layout=True)\n",
    "    #axarr = axarr.flatten()\n",
    "\n",
    "    #f.suptitle(f'Samples from /autofs/cluster/octdata2/users/epc28/veritas/output/real_data/nonlinearly-augmented', fontsize=15)\n",
    "    #axarr[0].imshow(np.max(y, axis=0), cmap='magma')\n",
    "    #axarr[1].imshow(np.max(y, axis=1), cmap='magma')\n",
    "    #axarr[2].imshow(np.max(y, axis=2), cmap='magma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_params = {\n",
    "    \"oct_vol\": \"output/models/version_2/predictions/dylan_data/I_mosaic_1_1_0.nii\",\n",
    "    #\"oct_vol\": \"/cluster/octdata/users/cmagnain/190312_I46_SomatoSensory/I46_Somatosensory_20um_crop.nii\",\n",
    "    \"ground_truth\": \"/autofs/cluster/octdata2/users/epc28/veritas/output/models/version_2/predictions/dylan_data/ground_truth.nii\",\n",
    "    #\"ground_truth\": \"/autofs/cluster/octdata2/users/epc28/veritas/output/models/version_2/predictions/caroline_data/ground_truth.nii\",\n",
    "    \"thresh_start\": 0.05,\n",
    "    \"thresh_stop\": 0.95,\n",
    "    \"thresh_step\": 0.05,\n",
    "    \"metric\": \"dice\"\n",
    "}\n",
    "\n",
    "test_params = {\n",
    "    \"step_size\": 256,\n",
    "    \"threshold\": None,\n",
    "    \"checkpoint\": \"/autofs/cluster/octdata2/users/epc28/veritas/output/models/version_2/checkpoints/epoch=117-val_loss=0.00095.ckpt\",\n",
    "    \"model\": \"/autofs/cluster/octdata2/users/epc28/veritas/output/models/version_2\",\n",
    "}\n",
    "\n",
    "\n",
    "class OctVolume(Dataset):\n",
    "\n",
    "    def __init__(self, volume_path, trainee, tile_size, step_size, subset=-1, transform=None, target_transform=None):\n",
    "        self.volume_path = volume_path\n",
    "        self.device = 'cuda'\n",
    "        self.tile_size = tile_size\n",
    "        self.step_size = step_size\n",
    "        self.volume_dtype = torch.float\n",
    "        self.imprint_dtype = torch.float\n",
    "        self.trainee = trainee\n",
    "        \n",
    "        # Get all volume specific things\n",
    "        with torch.no_grad():\n",
    "            self.volume_nifti = nib.load(self.volume_path)\n",
    "            self.volume_affine = self.volume_nifti.affine\n",
    "            self.volume_header = self.volume_nifti.header\n",
    "            self.volume_tensor = torch.tensor(self.volume_nifti.get_fdata(), dtype=self.volume_dtype) # device=self.device\n",
    "            self.raw_volume_shape = self.volume_tensor.shape    \n",
    "        # Pad each dimension individually\n",
    "        self.pad_dimension()\n",
    "        self.imprint_tensor = torch.zeros(self.volume_tensor.shape, dtype=self.imprint_dtype, device=self.device)\n",
    "        # Partition volume into overlapping 3d patches\n",
    "        self.get_frame_coords(step_size=self.step_size)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coordlist)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        working_coords = self.coordlist[idx]\n",
    "        x_slice = slice(*working_coords[0])\n",
    "        y_slice = slice(*working_coords[1])\n",
    "        z_slice = slice(*working_coords[2])\n",
    "        tile = self.volume_tensor[x_slice, y_slice, z_slice].to(self.volume_dtype).detach()\n",
    "        prediction = self.trainee(tile.unsqueeze(0).unsqueeze(0).to('cuda'))\n",
    "        prediction = torch.sigmoid(prediction).squeeze().squeeze().detach()\n",
    "        self.imprint_tensor[x_slice, y_slice, z_slice] += prediction\n",
    "        return tile, prediction\n",
    "\n",
    "\n",
    "    def predict(self):\n",
    "        '''Predict on all patches within 3d volume via getitem function. Normalize resultant imprint and strip padding.'''\n",
    "        # Normalizing\n",
    "        self.volume_tensor = cc.QuantileTransform(pmin=0, pmax=1, vmin=0.05, vmax=0.95, clamp=False)(self.volume_tensor + 0.000001)\n",
    "        length = self.__len__()\n",
    "        print(\"Predicting on\", length, 'patches')\n",
    "        for i in range(length):\n",
    "            self.__getitem__(i)\n",
    "            sys.stdout.write(f\"\\rPrediction {i + 1}/{length}\")\n",
    "            sys.stdout.flush()\n",
    "        s = slice(self.tile_size, -self.tile_size)\n",
    "        self.volume_tensor = self.volume_tensor[s, s, s]\n",
    "        self.imprint_tensor = self.imprint_tensor[s, s, s]\n",
    "\n",
    "\n",
    "    def pad_dimension(self):\n",
    "        with torch.no_grad():\n",
    "            self.volume_tensor = self.volume_tensor.clone().detach().unsqueeze(0)\n",
    "            if len(self.volume_tensor.shape) == 4:\n",
    "                padding = torch.ones(1, 6, dtype=torch.int) * self.tile_size\n",
    "                padding = tuple(*padding)\n",
    "                self.volume_tensor = torch.nn.functional.pad(self.volume_tensor, padding, 'replicate').squeeze()\n",
    "            else:\n",
    "                print('Input tensor has shape', self.volume_tensor.shape)\n",
    "\n",
    "\n",
    "    def get_frame_coords(self, step_size):\n",
    "        coords = []\n",
    "        for dim in range(3):\n",
    "            dim_start_frame = list(np.arange(0, self.volume_tensor.shape[dim] - self.tile_size, step_size))\n",
    "            # Remove all elements from starting frame list if all they're going to get is padding\n",
    "            dim_start_frame.remove(0)\n",
    "            # Remove all elements from starting frame list if all they're going to get is padding\n",
    "            dim_end_frame = [d + self.tile_size for d in dim_start_frame]\n",
    "            coords.append(zip(dim_start_frame, dim_end_frame))\n",
    "        for dim in range(len(coords)):\n",
    "            if dim == 0:\n",
    "                self.x_coords = [i for i in coords[dim]]\n",
    "            if dim == 1:\n",
    "                self.y_coords = [i for i in coords[dim]]\n",
    "            if dim == 2:\n",
    "                self.z_coords = [i for i in coords[dim]]\n",
    "        self.coordlist = []\n",
    "        try:\n",
    "            for x in self.x_coords:\n",
    "                for y in self.y_coords:\n",
    "                    for z in self.z_coords:\n",
    "                        self.coordlist.append([x, y, z])\n",
    "        except:\n",
    "            for x in self.x_coords:\n",
    "                for y in self.y_coords:\n",
    "                    self.coordlist.append([x, y])\n",
    "\n",
    "\n",
    "def findthresh(prediction, ground_truth):\n",
    "    prediction = prediction / torch.max(prediction)\n",
    "    threshold_lst = np.arange(vol_params[\"thresh_start\"], vol_params[\"thresh_stop\"], vol_params[\"thresh_step\"])\n",
    "    lst = []\n",
    "    for thresh in threshold_lst:\n",
    "        prediction_temp = prediction.clone()\n",
    "        prediction_temp[prediction_temp >= thresh] = 1\n",
    "        prediction_temp[prediction_temp < thresh] = 0\n",
    "        if vol_params[\"metric\"] == 'dice':\n",
    "            metric = dice(prediction_temp, ground_truth, multiclass=False)\n",
    "        elif vol_params[\"metric\"] == \"iou\":\n",
    "            metric = jaccard_index(preds=prediction_temp, target=ground_truth, task=\"binary\")\n",
    "        else:\n",
    "            print(\"I don't know that metric!\")\n",
    "        lst.append(metric.tolist())\n",
    "    mx = max(lst)\n",
    "    mx_idx = lst.index(mx)\n",
    "    return threshold_lst[mx_idx], lst[mx_idx]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    factors = {256 : 0,\n",
    "               128: 1,\n",
    "               64: 2}\n",
    "    \n",
    "    averaging_factor = 1 / (8 ** factors[test_params[\"step_size\"]])\n",
    "    print(\"Averaging by:\", averaging_factor)\n",
    "\n",
    "    t1 = time.time()\n",
    "    \n",
    "    volume_path = vol_params[\"oct_vol\"]\n",
    "    model_path = test_params[\"model\"]\n",
    "\n",
    "    unet = models.UNet(model_path, test_params[\"checkpoint\"])\n",
    "    oct = OctVolume(volume_path, unet.trainee, tile_size=unet.model_params['data']['shape'], step_size=test_params[\"step_size\"])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        oct.predict()\n",
    "        #x, y = oct.volume_tensor.cpu().numpy(), oct.imprint_tensor.cpu().numpy()\n",
    "        x = oct.volume_tensor\n",
    "        # Averaging the imprint tensor (linear norm) from 0-1\n",
    "        y = oct.imprint_tensor * averaging_factor\n",
    "        savedir = f\"{model_path}/predictions/caroline_data\"\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "        \n",
    "        if vol_params[\"ground_truth\"] is None:\n",
    "            if test_params[\"threshold\"] is None:\n",
    "                nifti = nib.nifti1.Nifti1Image(y.cpu().numpy(), affine=oct.volume_affine, header=oct.volume_header)\n",
    "                out_file = f\"prediction_stepsz-{test_params['step_size']}_prob-map.nii\"\n",
    "            elif isinstance(test_params[\"threshold\"], float):\n",
    "                threshold = test_params['threshold']\n",
    "                human_threshold = round(threshold, 3)\n",
    "                y[y >= threshold] = 1\n",
    "                y[y < threshold] = 0\n",
    "                nifti = nib.nifti1.Nifti1Image(y.cpu().numpy(), affine=oct.volume_affine, header=oct.volume_header)\n",
    "                out_file = f\"prediction_stepsz-{test_params['step_size']}_thresh-{human_threshold}.nii\"\n",
    "            else:\n",
    "                print(\"I don't know what that threshold is!!!\")\n",
    "\n",
    "        elif vol_params[\"ground_truth\"] is not None:\n",
    "            ground_truth = f\"{savedir}/ground_truth.nii\"\n",
    "            ground_truth_nifti = nib.load(ground_truth)\n",
    "            ground_truth_tensor = torch.tensor(ground_truth_nifti.get_fdata(), dtype=torch.int).to('cuda')\n",
    "            ground_truth_tensor[ground_truth_tensor >= 1] = 1\n",
    "\n",
    "            if test_params[\"threshold\"] == None:\n",
    "                best_threshold, best_acc = findthresh(y, ground_truth_tensor)\n",
    "                threshold = best_threshold\n",
    "                accuracy = best_acc\n",
    "                human_accuracy = round(best_acc, 3)\n",
    "                human_threshold = round(best_threshold, 3)\n",
    "                \n",
    "                y[y >= threshold] = 1\n",
    "                y[y < threshold] = 0\n",
    "\n",
    "            elif isinstance(test_params['threshold'], float):\n",
    "                threshold = test_params['threshold']\n",
    "                human_threshold = round(threshold, 3)\n",
    "                # Thresholding according to human input of threshold\n",
    "                y[y >= threshold] = 1\n",
    "                y[y < threshold] = 0\n",
    "                if vol_params[\"metric\"] == 'dice':\n",
    "                    accuracy = dice(preds=y, target=ground_truth_tensor, multiclass=False)\n",
    "                elif vol_params[\"metric\"] == \"iou\":\n",
    "                    accuracy = jaccard_index(preds=y, target=ground_truth_tensor, task=\"binary\")\n",
    "                else:\n",
    "                    print(\"I don't know that metric!\")\n",
    "                human_accuracy = round(accuracy.item(), 3)\n",
    "            else:\n",
    "                print(\"I don't know what that threshold is!\")\n",
    "\n",
    "            print(\"\\nThreshold =\", human_threshold)\n",
    "            print(f'\\n{vol_params[\"metric\"]} =', human_accuracy)\n",
    "\n",
    "            nifti = nib.nifti1.Nifti1Image(y.cpu().numpy(), affine=oct.volume_affine, header=oct.volume_header)\n",
    "            out_file = f\"prediction_stepsz-{test_params['step_size']}_thresh-{human_threshold}_{vol_params['metric']}-{human_accuracy}.nii\"        \n",
    "\n",
    "        print('\\n', f\"Saving to {out_file}\")\n",
    "        nib.save(nifti , f\"{model_path}/predictions/{out_file}\")\n",
    "\n",
    "    t2 = time.time()\n",
    "    print(f\"\\nProcess took {round(t2 - t1, 2)} [sec]\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vesselsynth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
